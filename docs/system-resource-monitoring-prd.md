# 시스템 리소스 모니터링 PRD

## 1. 개요

### 1.1 목적
서버의 시스템 리소스(CPU, 메모리, 디스크, 네트워크)를 실시간으로 모니터링하여 시스템 상태를 파악하고, 리소스 부족이나 비정상 동작을 조기에 감지하는 모니터링 시스템을 구축합니다.

### 1.2 배경
- 서버 장애의 주요 원인은 리소스 고갈 (CPU 과부하, 메모리 부족, 디스크 포화 등)
- 실시간 모니터링을 통해 장애 예방 및 신속한 대응 필요
- 리소스 사용 패턴 분석을 통한 용량 계획 수립

### 1.3 목표
- 시스템 리소스 실시간 수집 및 저장
- 직관적인 대시보드를 통한 시각화
- 임계값 기반 알림 시스템
- 과거 데이터 분석을 통한 트렌드 파악

## 2. 주요 기능

### 2.1 CPU 모니터링

#### 2.1.1 수집 메트릭
- **전체 CPU 사용률 (%)**: 시스템 전체의 CPU 사용률
- **코어별 CPU 사용률 (%)**: 각 CPU 코어의 개별 사용률
- **로드 애버리지**: 1분, 5분, 15분 평균 로드
- **프로세스별 CPU 사용률**: Top N 프로세스의 CPU 점유율
- **CPU 컨텍스트 스위칭 횟수**: 초당 컨텍스트 스위칭 빈도
- **User/System/Idle/IOWait 비율**: CPU 시간 분할 상세 정보

#### 2.1.2 수집 주기
- 기본: 10초
- 설정 가능 범위: 5초 ~ 60초

#### 2.1.3 알림 조건
- CPU 사용률 > 80% (5분 지속)
- CPU 사용률 > 95% (1분 지속)
- 로드 애버리지 > CPU 코어 수 × 2

### 2.2 메모리 모니터링

#### 2.2.1 수집 메트릭
- **전체 메모리 (GB)**: 시스템 총 메모리 용량
- **사용 중인 메모리 (GB, %)**: 실제 사용 중인 메모리
- **사용 가능한 메모리 (GB, %)**: 사용 가능한 메모리
- **버퍼/캐시 메모리 (GB)**: 커널이 사용하는 버퍼 및 캐시
- **스왑 메모리**: 총 스왑 용량 및 사용률
- **프로세스별 메모리 사용량**: Top N 프로세스의 메모리 점유율
- **메모리 페이지 폴트 횟수**: 초당 페이지 폴트 발생 빈도

#### 2.2.2 수집 주기
- 기본: 10초
- 설정 가능 범위: 5초 ~ 60초

#### 2.2.3 알림 조건
- 메모리 사용률 > 85%
- 메모리 사용률 > 95% (긴급)
- 스왑 사용률 > 50%
- 사용 가능한 메모리 < 500MB

### 2.3 디스크 모니터링

#### 2.3.1 수집 메트릭
- **디스크 사용량**: 파티션별 사용 용량 및 사용률 (%)
- **디스크 읽기 속도 (MB/s)**: 초당 읽기 처리량
- **디스크 쓰기 속도 (MB/s)**: 초당 쓰기 처리량
- **IOPS**: 초당 입출력 작업 수 (읽기/쓰기 분리)
- **디스크 I/O 대기 시간 (ms)**: 평균 I/O 응답 시간
- **디스크 큐 길이**: 대기 중인 I/O 요청 수
- **Inode 사용률 (%)**: 파일시스템 Inode 사용률

#### 2.3.2 수집 주기
- 디스크 사용량: 60초
- I/O 성능 메트릭: 10초

#### 2.3.3 알림 조건
- 디스크 사용률 > 80%
- 디스크 사용률 > 90% (긴급)
- Inode 사용률 > 90%
- 디스크 I/O 대기 시간 > 100ms (지속)

### 2.4 네트워크 모니터링

#### 2.4.1 수집 메트릭
- **인바운드 트래픽 (Mbps)**: 수신 네트워크 대역폭 사용률
- **아웃바운드 트래픽 (Mbps)**: 송신 네트워크 대역폭 사용률
- **패킷 수**: 초당 수신/송신 패킷 수
- **패킷 손실률 (%)**: 드롭된 패킷 비율
- **에러 패킷 수**: 에러가 발생한 패킷 수
- **활성 연결 수**: 현재 활성화된 TCP 연결 수
- **연결 상태별 분포**: ESTABLISHED, TIME_WAIT, CLOSE_WAIT 등
- **네트워크 인터페이스별 통계**: 각 NIC별 메트릭

#### 2.4.2 수집 주기
- 기본: 10초
- 설정 가능 범위: 5초 ~ 60초

#### 2.4.3 알림 조건
- 네트워크 대역폭 사용률 > 80%
- 패킷 손실률 > 1%
- TIME_WAIT 연결 수 > 10,000

## 3. 기술 스택

### 3.1 메트릭 수집
- **Node.js**: 메인 애플리케이션 런타임
- **시스템 모니터링 라이브러리**:
  - `systeminformation`: 크로스 플랫폼 시스템 정보 수집
  - `os` (내장 모듈): 기본 OS 정보
  - `process` (내장 모듈): 프로세스 정보
- **대안**: `node-os-utils`, `pidusage`

### 3.2 데이터 저장
- **시계열 데이터베이스**:
  - InfluxDB (권장): 시계열 데이터 최적화
  - Prometheus: 메트릭 저장 및 쿼리
  - TimescaleDB: PostgreSQL 기반 시계열 확장
- **데이터 보관 정책**:
  - 원본 데이터: 7일
  - 1분 평균: 30일
  - 1시간 평균: 1년

### 3.3 시각화
- **Grafana**: 대시보드 및 차트 생성
- **대안**:
  - Chart.js + 커스텀 웹 대시보드
  - Kibana (ELK 스택 사용 시)

### 3.4 알림
- **알림 채널**:
  - Slack Webhook
  - 이메일 (SMTP)
  - PagerDuty (운영 환경)
  - Discord Webhook
- **알림 로직**: Grafana Alerts 또는 커스텀 알림 엔진

## 4. 시스템 아키텍처

### 4.1 구성 요소

```
┌─────────────────┐
│   Target Server │
│                 │
│  ┌───────────┐  │
│  │ Collector │  │ ← 메트릭 수집 에이전트
│  │  Agent    │  │
│  └─────┬─────┘  │
└────────┼────────┘
         │ HTTP/gRPC
         ▼
┌─────────────────┐
│  Metric Store   │
│   (InfluxDB)    │ ← 시계열 데이터베이스
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│    Grafana      │ ← 시각화 및 알림
│   Dashboard     │
└─────────────────┘
```

### 4.2 데이터 플로우
1. **수집**: Collector Agent가 시스템 메트릭 수집
2. **전송**: HTTP API 또는 gRPC를 통해 중앙 서버로 전송
3. **저장**: InfluxDB에 시계열 데이터로 저장
4. **쿼리**: Grafana가 InfluxDB 쿼리하여 대시보드 렌더링
5. **알림**: 임계값 초과 시 알림 채널로 통지

## 5. API 설계

### 5.1 메트릭 전송 API

```http
POST /api/v1/metrics
Content-Type: application/json

{
  "server_id": "web-server-01",
  "timestamp": "2026-02-02T14:30:00Z",
  "metrics": {
    "cpu": {
      "usage_percent": 45.2,
      "cores": [35.1, 42.3, 50.8, 43.2],
      "load_average": [1.5, 1.8, 2.1]
    },
    "memory": {
      "total_gb": 16,
      "used_gb": 8.5,
      "used_percent": 53.1,
      "available_gb": 7.5,
      "swap_used_percent": 0
    },
    "disk": {
      "/": {
        "total_gb": 100,
        "used_gb": 65,
        "used_percent": 65,
        "read_mbps": 12.5,
        "write_mbps": 8.3
      }
    },
    "network": {
      "eth0": {
        "rx_mbps": 25.6,
        "tx_mbps": 18.2,
        "rx_packets": 15234,
        "tx_packets": 12456
      }
    }
  }
}
```

### 5.2 메트릭 조회 API

```http
GET /api/v1/metrics?server_id=web-server-01&metric=cpu.usage_percent&from=2026-02-02T00:00:00Z&to=2026-02-02T23:59:59Z
```

## 6. 대시보드 설계

### 6.1 메인 대시보드
- **서버 목록**: 모든 모니터링 대상 서버 상태 (정상/경고/위험)
- **전체 개요**: 클러스터 전체의 평균 CPU, 메모리, 디스크, 네트워크
- **알림 현황**: 최근 알림 목록 및 미확인 알림 수

### 6.2 서버 상세 대시보드
- **CPU**:
  - 시간별 사용률 그래프
  - 코어별 히트맵
  - Top 10 프로세스 테이블
- **메모리**:
  - 메모리 사용량 스택 차트 (Used/Buffer/Cache/Free)
  - 스왑 사용량 그래프
  - Top 10 메모리 사용 프로세스
- **디스크**:
  - 파티션별 사용률 게이지
  - I/O 처리량 그래프 (읽기/쓰기)
  - IOPS 그래프
- **네트워크**:
  - 인바운드/아웃바운드 트래픽 그래프
  - 패킷 손실률 그래프
  - 활성 연결 수 그래프

## 7. 구현 계획

### Phase 1: MVP (2주)
- [x] 기본 메트릭 수집 에이전트 구현
  - CPU, 메모리, 디스크, 네트워크 기본 메트릭
- [ ] InfluxDB 연동
- [ ] 메트릭 전송 API 구현
- [ ] 기본 Grafana 대시보드 구성

### Phase 2: 고급 기능 (2주)
- [ ] 프로세스별 상세 메트릭
- [ ] 알림 시스템 구현 (Slack 연동)
- [ ] 멀티 서버 지원
- [ ] 대시보드 개선 및 최적화

### Phase 3: 안정화 및 확장 (1주)
- [ ] 성능 최적화
- [ ] 에러 핸들링 강화
- [ ] 문서화
- [ ] 단위 테스트 작성

## 8. 성능 요구사항

### 8.1 에이전트 리소스 사용량
- CPU 사용률: < 5%
- 메모리 사용량: < 100MB
- 네트워크 대역폭: < 1Mbps

### 8.2 데이터 수집
- 수집 주기: 10초 (설정 가능)
- 메트릭 전송 지연: < 1초
- 배치 전송: 최대 100개 메트릭

### 8.3 데이터베이스
- 쿼리 응답 시간: < 500ms (24시간 데이터)
- 데이터 보관: 7일 (원본), 30일 (집계)

## 9. 보안 고려사항

- API 엔드포인트 인증 (API Key 또는 JWT)
- HTTPS/TLS 통신
- 민감한 정보 (프로세스 명령줄 인자) 필터링
- Rate Limiting 적용
- 서버 ID 검증

## 10. 모니터링 및 운영

### 10.1 메타 모니터링
- 에이전트 상태 모니터링 (Heartbeat)
- 메트릭 전송 실패율
- 데이터베이스 연결 상태

### 10.2 로깅
- 에이전트 동작 로그
- API 요청/응답 로그
- 에러 로그 (스택 트레이스 포함)

## 11. 향후 확장 계획

- 컨테이너 메트릭 (Docker, Kubernetes)
- 클라우드 리소스 모니터링 (AWS CloudWatch, GCP Monitoring)
- 예측 분석 (ML 기반 이상 탐지)
- 분산 추적 연동 (OpenTelemetry)
- 커스텀 메트릭 수집 플러그인 시스템

## 12. 참고 자료

- [systeminformation npm package](https://www.npmjs.com/package/systeminformation)
- [InfluxDB Documentation](https://docs.influxdata.com/)
- [Grafana Documentation](https://grafana.com/docs/)
- [Prometheus Best Practices](https://prometheus.io/docs/practices/naming/)
